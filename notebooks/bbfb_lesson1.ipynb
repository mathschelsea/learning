{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kup5jz-1S36r"
      },
      "source": [
        "# Lesson One"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the notebook I would have wanted access to when I started my Data Science journey. It is a concoction of my own learnings and those I picked up when I completed the Fast.ai [Introduction to Machine Learning for Coders](https://course18.fast.ai/ml.html) course back in 2018.\n",
        "\n",
        "This notebook has been setup for both VS Code users and Google Colab users. If you're a beginner I would recommend going down the Colab route."
      ],
      "metadata": {
        "id": "WJI95Mr87rmD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbRXBEM-S36r"
      },
      "source": [
        "## 0 - Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmU6BjjdS36s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pandas.api.types import is_string_dtype, is_object_dtype, is_numeric_dtype, is_datetime64_any_dtype\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# determines whether to import from colab or vscode\n",
        "editor = 'colab'\n",
        "\n",
        "# repository location on Google Drive\n",
        "drive_path = '/content/gdrive/MyDrive/learning'"
      ],
      "metadata": {
        "id": "zYhJi24WV5Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - VSCode Import"
      ],
      "metadata": {
        "id": "yQ3FVlgcS7fn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YaeK1huS36s"
      },
      "source": [
        "The code below does the following:\n",
        "1. Installs Kaggle\n",
        "2. Creates a Kaggle folder in our home directory (it'll be hidden)\n",
        "3. Gets our API credentials from the Kaggle 'Settings' page\n",
        "4. Places the credentials (.json file) in the Kaggle folder from step 2\n",
        "5. Downloads the Kaggle dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if editor == 'vscode':\n",
        "  # change current working directory\n",
        "  os.chdir('..')\n",
        "  print(f'cwd: {os.getcwd()}')\n",
        "\n",
        "  # install Kaggle\n",
        "  !pip install -q kaggle\n",
        "\n",
        "  # create a kaggle directory\n",
        "  dir = os.path.expanduser('~/.kaggle')\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  # copy credentials to kaggle folder\n",
        "  creds = '/Users/chelseatucker/credentials/kaggle.json'\n",
        "  !cp $creds ~/.kaggle\n",
        "\n",
        "  # change permissions so only I have read & write access to the credentials file\n",
        "  !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "  # create a bulldozers directory\n",
        "  os.makedirs('data/bbfb', exist_ok=True)\n",
        "\n",
        "  # downloading the bulldozers dataset to the 'data' folder\n",
        "  !kaggle competitions download -c bluebook-for-bulldozers -p 'data/bbfb'\n",
        "\n",
        "  # unzip the data\n",
        "  !unzip -q data/bbfb/bluebook-for-bulldozers.zip -d 'data/bbfb'\n",
        "\n",
        "  # unzip train data\n",
        "  !unzip -q data/bbfb/Train.zip -d 'data/bbfb'"
      ],
      "metadata": {
        "id": "0kwukEUdWBOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Colab Import"
      ],
      "metadata": {
        "id": "9zMF-vLPTLAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first section of code mounts Google Drive and navigates to where the cloned repository sits. The second section downloads the data from Kaggle in a simialr way to that in the '1 - VSCode Import' section."
      ],
      "metadata": {
        "id": "q9xkAh9DlsTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if editor == 'colab':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  # navigate to the repository\n",
        "  %cd $drive_path"
      ],
      "metadata": {
        "id": "_oi6QxW0lLYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if editor == 'colab':\n",
        "  if os.path.exists('data/bbfb'):\n",
        "    print('Bulldozers data already present on Google Drive')\n",
        "  else:\n",
        "    # install kaggle\n",
        "    !pip install -q kaggle\n",
        "\n",
        "    # upload the 'Kaggle.json' file\n",
        "    from google.colab import files\n",
        "    files.upload()\n",
        "\n",
        "    # make a kaggle directory and move the json file there\n",
        "    !mkdir ~/.kaggle\n",
        "    !mv kaggle.json ~/.kaggle\n",
        "\n",
        "    # change permissions so only I have read & write access to the credentials file\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    # download dataset from Kaggle\n",
        "    !kaggle competitions download -c 'bluebook-for-bulldozers'\n",
        "\n",
        "    # move dataset\n",
        "    !mkdir data\n",
        "    !mv bluebook-for-bulldozers.zip data\n",
        "\n",
        "    # unzip bulldozers data\n",
        "    !unzip data/bluebook-for-bulldozers.zip -d data/bbfb\n",
        "\n",
        "    # unzip train data\n",
        "    !unzip data/bbfb/Train.zip -d data/bbfb"
      ],
      "metadata": {
        "id": "RLmWrupoWX7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZZyPYxaS36t"
      },
      "source": [
        "## 2 - Exploring the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYsN7KvpS36t"
      },
      "outputs": [],
      "source": [
        "df_raw = pd.read_csv('data/bbfb/Train.csv',\n",
        "                     low_memory=False,\n",
        "                     parse_dates=['saledate'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17L44oGMS36u"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# quick look to see if the data has imported correctly\n",
        "df_raw.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsRbcqyZS36u"
      },
      "outputs": [],
      "source": [
        "from utils.eda import df_look\n",
        "\n",
        "# high level overview of the data\n",
        "df_look(df_raw)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking individual columns\n",
        "df_raw.Tire_Size.value_counts(dropna=False).sort_index()"
      ],
      "metadata": {
        "id": "dhWowMfJQ-4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNWrseGJS36u"
      },
      "source": [
        "## 3 - Data Cleansing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Corrections"
      ],
      "metadata": {
        "id": "aa8gIJq9RKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing certain values\n",
        "df0 = df_raw.copy()\n",
        "\n",
        "# different naming conventions for the same level\n",
        "# e.g. '10 inch' and '10'\n",
        "# editing so the levels has consistent naming conventions\n",
        "# also converting to float\n",
        "df0.Tire_Size = df0.Tire_Size.str.replace('\"','')\n",
        "df0.Tire_Size = df0.Tire_Size.str.replace(' inch','')\n",
        "\n",
        "# tidying up '31.5 inch' level\n",
        "# also converting to int\n",
        "df0.Undercarriage_Pad_Width = df0.Undercarriage_Pad_Width.str.replace('.5','')\n",
        "df0.Undercarriage_Pad_Width = df0.Undercarriage_Pad_Width.str.replace(' inch','')\n",
        "\n",
        "# different naming conventions for the same level\n",
        "# editing so the levels has consistent naming conventions\n",
        "df0.Transmission = df0.Transmission.str.replace('AutoShift', 'Autoshift')"
      ],
      "metadata": {
        "id": "BCsLIbzZRNMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Converting Data Types"
      ],
      "metadata": {
        "id": "uFQUoMFBV6_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# manually changing these columns to numerical\n",
        "df0.Tire_Size.fillna('0', inplace=True)\n",
        "df0.Tire_Size = df0.Tire_Size.str.replace('None or Unspecified', '-1')\n",
        "df0.Tire_Size = df0.Tire_Size.astype(float)\n",
        "\n",
        "df0.Undercarriage_Pad_Width.fillna('0', inplace=True)\n",
        "df0.Undercarriage_Pad_Width = df0.Undercarriage_Pad_Width.str.replace('None or Unspecified', '-1')\n",
        "df0.Undercarriage_Pad_Width = df0.Undercarriage_Pad_Width.astype(int)\n",
        "\n",
        "df0.Blade_Width = df0.Blade_Width.str.replace(\"'\",\"\")\n",
        "df0.Blade_Width = df0.Blade_Width.str.replace(\"<12\",\"11\")\n",
        "df0.Blade_Width.fillna('0', inplace=True)\n",
        "df0.Blade_Width = df0.Blade_Width.str.replace('None or Unspecified', '-1')\n",
        "df0.Blade_Width = df0.Blade_Width.astype(int)"
      ],
      "metadata": {
        "id": "0DV570KnV6fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Missing Values"
      ],
      "metadata": {
        "id": "GG1qr5GXrFpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check percentage missing for each column\n",
        "df0.isnull().sum().sort_values(ascending=False)/len(df0)"
      ],
      "metadata": {
        "id": "R5H5OVB16tpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical missings\n",
        "for c in df0.columns:\n",
        "    if is_string_dtype(df0[c]) or is_object_dtype(df0[c]):\n",
        "        df0[c].fillna('Missing', inplace=True)"
      ],
      "metadata": {
        "id": "GFuTZj_ys5pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numerical missings\n",
        "mean = df0.MachineHoursCurrentMeter.mean()\n",
        "df0.MachineHoursCurrentMeter.fillna(mean, inplace=True)\n",
        "\n",
        "# taking the most common level\n",
        "common = df0.auctioneerID.value_counts().sort_values(ascending=False)\n",
        "df0.auctioneerID.fillna(common.index[0], inplace=True)"
      ],
      "metadata": {
        "id": "HrcxRsc4tgXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Categorical Variables"
      ],
      "metadata": {
        "id": "MGNnQOY9ZX-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# banding levels with less data together\n",
        "threshold = 50\n",
        "\n",
        "for c in ['fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor']:\n",
        "  counts = df0[c].value_counts(dropna=False)\n",
        "  flag = df0[c].isin(counts.index[counts < threshold])\n",
        "  df0.loc[flag, c] = 'Grouped'"
      ],
      "metadata": {
        "id": "hc_0KxwmT8GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list object/string columns\n",
        "cats = []\n",
        "for c in df0.columns:\n",
        "    if is_string_dtype(df0[c]) or is_object_dtype(df0[c]):\n",
        "        cats.append(c)\n",
        "\n",
        "cats"
      ],
      "metadata": {
        "id": "T2EQTaJqg9da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.preprocessing import conv_to_cat\n",
        "\n",
        "# converting all string/object columns to categories\n",
        "conv_to_cat(df0)\n",
        "\n",
        "# checking category orders\n",
        "for c in cats:\n",
        "    print(c,':',df0[c].cat.categories)\n",
        "    print()"
      ],
      "metadata": {
        "id": "xoj9Z5gQhNrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS4r_JueS36w"
      },
      "outputs": [],
      "source": [
        "# reordering categories\n",
        "df0.UsageBand = df0.UsageBand.cat.reorder_categories(\n",
        "    ['Missing',\n",
        "     'Low',\n",
        "     'Medium',\n",
        "     'High'], ordered=True)\n",
        "\n",
        "df0.ProductSize = df0.ProductSize.cat.reorder_categories(\n",
        "    ['Missing',\n",
        "     'Mini',\n",
        "     'Small',\n",
        "     'Compact',\n",
        "     'Medium',\n",
        "     'Large / Medium',\n",
        "     'Large'], ordered=True)\n",
        "\n",
        "df0.Drive_System = df0.Drive_System.cat.reorder_categories(\n",
        "    ['Missing',\n",
        "     'No',\n",
        "     'Two Wheel Drive',\n",
        "     'Four Wheel Drive',\n",
        "     'All Wheel Drive'], ordered=True)\n",
        "\n",
        "df0.Grouser_Type = df0.Grouser_Type.cat.reorder_categories(\n",
        "    ['Missing',\n",
        "     'Single',\n",
        "     'Double',\n",
        "     'Triple'], ordered=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking after reordering\n",
        "df0.UsageBand.cat.categories"
      ],
      "metadata": {
        "id": "1CIW4xaLhnYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Defining Feature Spec"
      ],
      "metadata": {
        "id": "q61gSqREZiaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 'feature spec' can be thought of as a blue print of your cleansed data. I create these so that I can easily apply the same set of edits and rules to training, validation and test datasets."
      ],
      "metadata": {
        "id": "jERZ8HwSAkPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [c for c in df0.columns if c != 'SalePrice']\n",
        "feat_dict = {}\n",
        "max_cat = 100\n",
        "\n",
        "# defining a default feature spec\n",
        "for col in features:\n",
        "  number = is_numeric_dtype(df0[col])\n",
        "  string = is_string_dtype(df0[col]) or is_object_dtype(df0[col])\n",
        "  date   = is_datetime64_any_dtype(df0[col])\n",
        "  cat_no = df0[col].nunique(dropna=False)\n",
        "\n",
        "  # default imputation\n",
        "  if number:\n",
        "    imputation = 0\n",
        "  elif string:\n",
        "    imputation = 'Missing'\n",
        "  else:\n",
        "    imputation = pd.to_datetime(\n",
        "        '2024-06-10 00:00:00.00',\n",
        "        format='%Y-%m-%d %H:%M:%S.%f')\n",
        "\n",
        "  # default categories\n",
        "  if string and cat_no < max_cat:\n",
        "    categories = list(df0[col].cat.categories)\n",
        "  else:\n",
        "    categories = []\n",
        "\n",
        "  feat_dict.update({col :\n",
        "   {'drop'      : 'N',\n",
        "    'imputation' : imputation,\n",
        "    'categories': categories,\n",
        "    'monotonic' : 'N'}})"
      ],
      "metadata": {
        "id": "ji0AGo44Zmw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# over-riding default values\n",
        "\n",
        "# drop\n",
        "for c in ['SalesID', 'MachineID', 'ModelID', 'saledate', 'fiModelDesc', 'fiBaseModel']:\n",
        "  feat_dict[c]['drop'] = 'Y'\n",
        "\n",
        "# imputation\n",
        "feat_dict['auctioneerID']['imputation'] = df0.auctioneerID.value_counts().sort_values(ascending=False).index[0]\n",
        "feat_dict['YearMade']['imputation'] = 1000\n",
        "feat_dict['MachineHoursCurrentMeter']['imputation'] = df0.MachineHoursCurrentMeter.mean()\n",
        "feat_dict['saledate']['imputation'] = df0.saledate.value_counts().sort_values(ascending=False).index[0]\n",
        "\n",
        "# monotonic\n",
        "for c in ['YearMade', 'MachineHoursCurrentMeter']:\n",
        "  feat_dict[c]['monotonic'] = 'Y'"
      ],
      "metadata": {
        "id": "oja7js5waE_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exporting the feature spec\n",
        "with open('feature_spec.yaml', 'w') as outfile:\n",
        "    yaml.dump(feat_dict, outfile, default_flow_style=False)"
      ],
      "metadata": {
        "id": "qO2kPWfaBQ0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Feature Engineering"
      ],
      "metadata": {
        "id": "-zSAOzUnwUhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Numericalise"
      ],
      "metadata": {
        "id": "DclUebNqbW3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.preprocessing import numericalise\n",
        "\n",
        "# converting all categorical columns to their code equivalents\n",
        "df1 = df0.copy()\n",
        "\n",
        "for c in cats:\n",
        "    numericalise(df1, df1[c], c, max_n_cat=max_cat)"
      ],
      "metadata": {
        "id": "DEhAdBVSbWSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Date Attributes"
      ],
      "metadata": {
        "id": "hLC6k9yeYn0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.preprocessing import add_dateattr\n",
        "\n",
        "# extracting more information from the date field\n",
        "add_dateattr(df1, 'saledate', drop=False)"
      ],
      "metadata": {
        "id": "YFr0rAWuZEZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Dependent Variable"
      ],
      "metadata": {
        "id": "NpHqRoe6wy7y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7XKcmxbS36u"
      },
      "outputs": [],
      "source": [
        "# The competiton wants us to use RMSLE as the measure between actuals and\n",
        "# predictions so we'll take the log of the dependent variable.\n",
        "df1.SalePrice = np.log(df1.SalePrice)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Final Edits"
      ],
      "metadata": {
        "id": "MjdIuRk_lCld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for c in features:\n",
        "  if feat_dict[c]['drop'] == 'Y':\n",
        "    df1.drop(c, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "E9JOIVSslFwh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}