{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVnaw97iHhygrp5Q9xrzBh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Whisper Notebook"
      ],
      "metadata": {
        "id": "IhzW4btBm-zB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1 - Notebook setup"
      ],
      "metadata": {
        "id": "KieN63M5nP3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following command will pull and install the latest commit from [OpenAI's Whisper repository](https://github.com/openai/whisper) along with its Python dependencies."
      ],
      "metadata": {
        "id": "X45H_FbRR23b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8KN4hn7RLPU"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll also want to set Colab's hardware accelerator to 'GPU'. You can do this by going to 'view resources' (available from the drop-down list next to the RAM/Disk bars) and then selecting 'change runtime type'."
      ],
      "metadata": {
        "id": "IfKosUOTguSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2 - High level model access"
      ],
      "metadata": {
        "id": "iox1HLqAnbfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - English to English Transcription"
      ],
      "metadata": {
        "id": "6Zyb8Hi1nwH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this sub-section we'll upload one or more audio files containing English speech and transcribe the content of that audio into English text. So first things first, let's upload the audio:"
      ],
      "metadata": {
        "id": "ueh1lPi5TREW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload() # run this to get an upload widget"
      ],
      "metadata": {
        "id": "nxBgZYe-VOvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll load Whisper and ask it to transcribe the audio file we just uploaded:"
      ],
      "metadata": {
        "id": "JAcVP1z2S3Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base.en\")\n",
        "result = model.transcribe(\"eleanor_oliphant_long.m4a\", language=\"en\", fp16=False)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "xAzK2Qx8TBHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 French to English Translation"
      ],
      "metadata": {
        "id": "FL6mF-w5tgPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this sub-section we'll upload one or more audio files containing French speech and translate the content of that audio into English text. Let's upload the audio:"
      ],
      "metadata": {
        "id": "Ojv_9KOutobx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload() # run this to get an upload widget"
      ],
      "metadata": {
        "id": "fApqDVM8txAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first see how Whisper fairs transcribing French speech to French text:"
      ],
      "metadata": {
        "id": "N7UToDadwlIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"amelie_original.m4a\", language='fr', fp16=False)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "yAi0qwfxyReQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how well it translates French speech to English text:"
      ],
      "metadata": {
        "id": "b7xKoolcraji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"amelie_original.m4a\", language='fr', task='translate', fp16=False)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "zPx1Bdmsu5Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the same as above but on a slightly more accurate model:"
      ],
      "metadata": {
        "id": "6qKcGeJwrp6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"small\")\n",
        "result = model.transcribe(\"amelie_original.m4a\", language='fr', task='translate', fp16=False)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "SXZvBUUPweWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3 - Low level model access"
      ],
      "metadata": {
        "id": "n_mdCiTqhsoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we'll look at some low level Whisper access using `whisper.decode()` and `whisper.detect_language()`:"
      ],
      "metadata": {
        "id": "P8KXY4rOngvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model('small')\n",
        "\n",
        "# load audio and pad/trim it to fit 30 seconds\n",
        "audio = whisper.load_audio('amelie_original.m4a')\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "# make log-Mel spectrogram and move to the same device as the model\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)"
      ],
      "metadata": {
        "id": "dZGdJyu4nnjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 - Language detection"
      ],
      "metadata": {
        "id": "X5u-SY14-hsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# detect the spoken language\n",
        "_, probs = model.detect_language(mel)\n",
        "lang = max(probs, key=probs.get)\n",
        "prob = \"{0:.0%}\".format(max(probs.values()))\n",
        "\n",
        "# print language that scored the highest liklihood\n",
        "print(f'Detected language (and probability): {lang}', f'({prob})')"
      ],
      "metadata": {
        "id": "yRVcZYAyqvyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 - French to English Translation"
      ],
      "metadata": {
        "id": "0ak10hLcB3wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decode the audio\n",
        "options = whisper.DecodingOptions(language='fr', task='translate')\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "# print the recognized text\n",
        "print(result.text)"
      ],
      "metadata": {
        "id": "INc50UfBqxgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}