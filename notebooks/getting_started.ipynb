{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MvDA6QwOjIfL",
        "MA3LXGxDjgR3",
        "2pZbAiXW3CoN",
        "DtIgHCu_2c0u",
        "pW1LTVG22hio",
        "EzjRIdhn898J"
      ],
      "authorship_tag": "ABX9TyNq3TJfIg3IFd7Ix/+fCw72"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started with Data Science"
      ],
      "metadata": {
        "id": "_tTpU3XfXTeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to get started on or reignite your Data Science journey then look no further than this notebook. In here you will do the following:\n",
        "\n",
        "1. Use a free, cloud based platform to run Python code.\n",
        "2. Download a Data Science dataset.\n",
        "3. Train your first machine learning model.\n",
        "\n",
        "This can all be done in less than 10 minutes. Let's get cracking!"
      ],
      "metadata": {
        "id": "06Gx0oIMi10h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Google Colab"
      ],
      "metadata": {
        "id": "MvDA6QwOjIfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The free, cloud based platform that I speak about above is Google Colaboratory - Colab for short.\n",
        "\n",
        "‚òÅÔ∏è Colab provides a convenient and free platform for running Python code, particularly suited for those not interested in local installation or powerful hardware. It's a hosted Jupyter Notebook service, inheriting all the pros of Jupyter, such as interactivity, visualizations, and documentation capabilities.\n",
        "\n",
        "You can open this notebook in Colab by clicking the button below."
      ],
      "metadata": {
        "id": "n3wDX37cXfS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mathschelsea/learning/blob/main/notebooks/getting_started.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "This then becomes your notebook to edit as you wish and save to your Google Drive. If you don't have a Google account you may need to open one (don't worry, that is also free and quick to set up)"
      ],
      "metadata": {
        "id": "gGApCGJWrg1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Kaggle Dataset"
      ],
      "metadata": {
        "id": "MA3LXGxDjgR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Data Science dataset is one provided by Kaggle.\n",
        "\n",
        "üî≠ Kaggle is a Data Science competition platform and online community. It provides a space to delve into diverse datasets, work on modelling projects, collaborate with others, and learn from shared insights.\n",
        "\n",
        "The below code downloads the Kaggle dataset [Blue Book for Bulldozers](https://www.kaggle.com/c/bluebook-for-bulldozers/overview). If you'd prefer a different dataset then feel free to edit the code below as required. If you don't have a [Kaggle](https://www.kaggle.com/) account then you will need to open one (again, don't worry, it's free and quick to open)."
      ],
      "metadata": {
        "id": "7iVm1Jc5X1eR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - Kaggle Credentials"
      ],
      "metadata": {
        "id": "PaHEsQR_moir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go the 'Settings' page on Kaggle and on the 'Account' tab scroll down to 'API'. Select the 'Create New Token' button (see image below). This will download a 'kaggle.json' file to your PC. Remember where this file is downloaded to as we'll need it later."
      ],
      "metadata": {
        "id": "bqinLXJEm-pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Kaggle Crednetials](https://raw.githubusercontent.com/mathschelsea/learning/main/media/kaggle_api.png)"
      ],
      "metadata": {
        "id": "ehDvR8_moqeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 - Download Data"
      ],
      "metadata": {
        "id": "8WWKpZnxpXmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below to download the Blue Book for Bulldozers data into your Colab runtime session. When you run the code you will be prompted to uploaded your Kaggle credentials into your Colab runtime. This is the 'kaggle.json' file that you downloaded in the previous section. You'll upload this so that it can be placed in the '.kaggle' directory that we create on your local machine. So effectively we're just moving your kaggle credentials to where they need to be for us to use the Kaggle dataset API."
      ],
      "metadata": {
        "id": "V8J7VEH5pccQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdiWBxm5XSyI"
      },
      "outputs": [],
      "source": [
        "# install kaggle\n",
        "!pip install -q kaggle\n",
        "\n",
        "# upload the 'Kaggle.json' file\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# make a kaggle directory and move the json file there\n",
        "!mkdir ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle\n",
        "\n",
        "# change permissions so only you have read & write access to the credentials\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# download dataset from Kaggle\n",
        "!kaggle competitions download -c 'bluebook-for-bulldozers'\n",
        "\n",
        "# make a 'data' directory and move the dataset there\n",
        "!mkdir data\n",
        "!mv bluebook-for-bulldozers.zip data\n",
        "\n",
        "# unzip bulldozers data\n",
        "!unzip data/bluebook-for-bulldozers.zip -d data/bbfb\n",
        "\n",
        "# unzip train data\n",
        "!unzip data/bbfb/Train.zip -d data/bbfb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you open the left-hand-side panel on Colab, you should be able to see the new 'data' folder and the Blue Book for Bulldozers zipped dataset within it (see screenshot below)."
      ],
      "metadata": {
        "id": "rZbsWjf8qAOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://raw.githubusercontent.com/mathschelsea/learning/main/media/colab_files.png' width='300'>"
      ],
      "metadata": {
        "id": "N8Wf9XAJu_0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Machine Learning"
      ],
      "metadata": {
        "id": "_en_372OqRKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 - Looking at the data"
      ],
      "metadata": {
        "id": "egUJFdUPqWoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First up, let's have a look at the data. We'll open the 'Train' dataset as it's good practice to never use or look at the validation or test datasets."
      ],
      "metadata": {
        "id": "0Cb38T3xs5tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the data\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data/bbfb/Train.csv', low_memory=False, parse_dates=['saledate'])"
      ],
      "metadata": {
        "id": "MWNVlyk2qO-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resetting the display options to max rows and columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# looking at the first 3 rows of data\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "bfHZrj-ttyz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at some key data characteristics\n",
        "print(f'No. of rows in dataset: {df.shape[0]}')\n",
        "print(f'No. of cols in dataset: {df.shape[1]}')\n",
        "print('')\n",
        "df.info(verbose=True)"
      ],
      "metadata": {
        "id": "CKeB1HKkvBsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 - Editing the data"
      ],
      "metadata": {
        "id": "CPseT-tYqZ-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage we would cleanse, edit, and engineer the data."
      ],
      "metadata": {
        "id": "qB-irRl2z_T9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.1 - Missing Values"
      ],
      "metadata": {
        "id": "2pZbAiXW3CoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check percentage missing for each column\n",
        "df.isnull().sum().sort_values(ascending=False)/len(df)"
      ],
      "metadata": {
        "id": "Y5yiy4gG28lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for object features, impute missing values with 'missing'\n",
        "from pandas.api.types import is_string_dtype, is_object_dtype\n",
        "\n",
        "for c in df.columns:\n",
        "    if is_string_dtype(df[c]) or is_object_dtype(df[c]):\n",
        "        df[c].fillna('missing', inplace=True)"
      ],
      "metadata": {
        "id": "Y7FBKrli3PUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# re-run the cell above to check the missing list again\n",
        "# only two more columns with missing data\n",
        "# both are numerical\n",
        "\n",
        "# machinehourscurrentmeter - impute with the mean\n",
        "mean = df.MachineHoursCurrentMeter.mean()\n",
        "df.MachineHoursCurrentMeter.fillna(mean, inplace=True)\n",
        "\n",
        "# auctioneerid - impute with most common level values\n",
        "common = df.auctioneerID.value_counts().sort_values(ascending=False)\n",
        "df.auctioneerID.fillna(common.index[0], inplace=True)"
      ],
      "metadata": {
        "id": "wSNCPxt73uNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2 - Datetime features"
      ],
      "metadata": {
        "id": "DtIgHCu_2c0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting numerical information from the datetime features\n",
        "attr = ['Year', 'Month', 'Day', 'Dayofweek', 'Dayofyear', 'Quarter']\n",
        "\n",
        "for n in attr:\n",
        "  df['saledate_' + n.lower()] = getattr(df['saledate'].dt, n.lower())"
      ],
      "metadata": {
        "id": "xhzyqIriqczn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick check\n",
        "df[['saledate_year', 'saledate_month', 'saledate_day',\n",
        "    'saledate_dayofweek', 'saledate_dayofyear', 'saledate_quarter']].head(5)"
      ],
      "metadata": {
        "id": "WCKXmPzH1gy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2 - Object Features"
      ],
      "metadata": {
        "id": "pW1LTVG22hio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting all object features to categorical features\n",
        "\n",
        "for n,c in df.items():\n",
        "  if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n",
        "  if is_object_dtype(c): df[n] = c.astype('category').cat.as_ordered()"
      ],
      "metadata": {
        "id": "G3Re99M_2TX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick spot check\n",
        "print('Categories:')\n",
        "print(df.Coupler_System.cat.categories)\n",
        "print()\n",
        "print('Value Counts:')\n",
        "print(df.Coupler_System.value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "v7KVc7HA5NmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now converting these categories to their equivalent code values\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "for c in df.columns:\n",
        "  if not is_numeric_dtype(c):\n",
        "      df[c] = pd.Categorical(df[c]).codes"
      ],
      "metadata": {
        "id": "fQVcbrGD5suc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick spot check\n",
        "print('Value Counts:')\n",
        "df.Coupler_System.value_counts(dropna=False).sort_index()"
      ],
      "metadata": {
        "id": "cLB0wN4z6g1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.3 - Drop Columns"
      ],
      "metadata": {
        "id": "EzjRIdhn898J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to drop any columns that we don't want or need."
      ],
      "metadata": {
        "id": "KUgn2eR-LCi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# looking for columns with nearly all unique levels (will drop these)\n",
        "print('Percentage Unique')\n",
        "for c in df.columns:\n",
        "  print(f'{c}:', '{:.1%}'.format(df[c].nunique()/len(df)))"
      ],
      "metadata": {
        "id": "n2JevPvqNf4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['SalesID', 'saledate', 'MachineID'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "EPHdNbku81X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 - Model Training"
      ],
      "metadata": {
        "id": "69-WY8G9qdg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.1 - Data Split"
      ],
      "metadata": {
        "id": "O7kcg2OM9NCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to split the data into a train and test split so we can train the model on the train split and then measure it's performance on the test split."
      ],
      "metadata": {
        "id": "oqRPbGfY9QQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining sizes\n",
        "test_size = 12000\n",
        "train_size = len(df) - test_size\n",
        "\n",
        "# splitting data\n",
        "def split_vals(a,n):\n",
        "  return a[:n].copy(), a[n:].copy()\n",
        "\n",
        "train_df, test_df = split_vals(df, train_size)\n",
        "\n",
        "# checking sizes\n",
        "train_df.shape, test_df.shape"
      ],
      "metadata": {
        "id": "O8kbLsNZ9Y8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.2 - Training"
      ],
      "metadata": {
        "id": "yWe-6w9C9_eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train a random forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "m = RandomForestRegressor(n_jobs=-1)\n",
        "m.fit(train_df.drop('SalePrice', axis=1), train_df.SalePrice)"
      ],
      "metadata": {
        "id": "iFD77WWlqhGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 - Evaluating"
      ],
      "metadata": {
        "id": "J1Vatmcbqkax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m.score(test_df.drop('SalePrice', axis=1), test_df.SalePrice)"
      ],
      "metadata": {
        "id": "5uSmWMxXqnJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Next Steps"
      ],
      "metadata": {
        "id": "onK1azttPwhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above showcased a quick way to train a machine learning model. The data exploration was very minimal and the engineering of the data was quite basic. A better model could be trained if more time and thought was put into these steps as well as others. Why don't you have a go at trying to improve the model performance value above by re-working this code. Here are some questions that you might want to answer and suggestions of things to do next.\n",
        "\n",
        "* Are there any errors in the data that could be corrected or filtered? Take a look a bit more at the features and their unique levels.\n",
        "* Are any of the features highly correlated? Is it reasonable to drop one of the correlated features from the training data?\n",
        "* Can we handle missing data better? Can more information be pulled out from the missing data?\n",
        "* Can more information be pulled out for the datetime fields? How do these extra features fair in a feature importance plot?\n",
        "* What is the 'score' metric in the section '3.4 - Evaluating'?\n",
        "* What other metrics could we use to evaluate a model's performance?\n",
        "* What is a feature importance plot? What feature has the highest importance in your model?\n",
        "* Why did we split the data in section '3.3.1 - Data Split'?.\n",
        "* Can you find out what your model's hyperparameters are? Do you know what each hyperparameter does?\n",
        "* Can you produce a lift plot? What does it show? What are other model explainability visuals you can use?\n",
        "* The model chosen is a random forest but others will do the job. Try out another model and see which performs better.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "onwpZOeLP0lD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XNAQJfq2Pz0j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}